# Поиск оптимальной стратегии игры в симуляторе владельца продукта с помощью Deep RL (DDQN)
<!-- Применение DDQN в симуляторе владельца продукта: оптимизация стратегий с помощью обучения с подкреплением (Chat GPT 4 version)-->

## Введение:

### Что такое Reinforcement Learning?

В обучении с подкреплением (reinforcement learning)
алгоритм учится
принимать последовательность решений
в некоторой среде
с целью максимизации суммарной награды.

<!-- Типичная картинка -->

### Что за симулятор владельца продукта?



## Актуальность

* Машинное обучение помогает человечеству автоматизировать и оптимизировать процессы
* В разработке очень важную роль играет управление рабочими задачами в продукте
* Давайте попробуем автоматизировать управление продукта на упрощенной модели мира

## Обзор существующих аналогов

### Есть ли еще игры - симуляторы владения продукта?
Game Dev Tycoon - игра, в которой вы создаете и развиваете свою собственную игровую компанию, начиная с 80-х годов.
<!--- Создается множество игр. Сами игры описыаются характеристиками тема, жанр, масштаб, технологии -->
Software Inc. - симулятор разработки программного обеспечения, в котором вы создаете и управляете своей собственной IT-компанией.
<!-- Очень сильный упор на реализм. -->

* Есть ли алгоритмы, которые уже хорошо играют в выбранную игру?
Точно нет алгоритмов обыгрывающих человека в Product Owner Simulator.

## Описание процесса достижение цели

Для успешного применения RL алгоритма к игре следует представить ее [игру] в виде среды с системой наград.
Среда должна иметь свойство марковости, то есть её поведение зависит только от текущего состояния и не зависит от предыдущих.

### Среда
Рассмотрим среду с которой будет взаимодействовать агент.
У среды можно определить пространство состояний, важна его конечность.
<!-- Скачек от пространства состояний к параметрам -->
Например, в выбранной игре один из параметров, описывающих состояние среды, — это номер спринта, который на практике не превышает ста.
Но в набор параметров так же входит количество пользователей и их лояльность, которые ведут себя как действительные числа.
Хоть в компьютере числа хранятся с конечной точностью, упомянутый фактор делает пространство игры необъятным, поэтому можно считать его бесконечным.

Аналогично определяется пространство действий.
В случае Симулятора владельца продукта есть конечное число заранее заданных действий, например: начать спринт, приобрести работника или начать исследование.
В ходе игры можно передвигать карточки, количество которых величина динамическая.
В рамках игры количество карточек ограничено небольшим числом, поэтому пространство действий конечно.

В данной среде присутству случайные события, например, прирост пользователей благодаря релизу задачи или возникновения бага при релизе.

В игре можно выиграть, победить - набрать миллион условных денег или проиграть - опуститься до нуля.
Таким образом среда обладает свойством эпизодичности.
Некоторые среды таким свойством не обладают, например, управление роботом или торговля на бирже.
Из-за отсутствия эпизодичности некоторые алгоритмы RL могут быть не применимы (Monte-Carlo) или плохо сходиться.

### Агент

Агент представляет собой алгоритм, выдающий оптимальное действие по состоянию среды.
Существует множество способов получения такого агента: Cross-Entropy, Policy Iteration, Monte-Carlo, SARSA, Q-Learning. <!-- Ссылка на ods.ai -->
Из перечисленных вариантов был выбран Q-Learning, он хорошо показывает себя в бесконечных средах с конечным пространством действий.
Учитывает стохастику и сходится быстрее чем Cross-Entropy.
<!-- Алгоритм не учитывает эпизодичность среды, это может замедлить обучени алгоритма, но не повлияет на его сходимость. -->

Для среды с бесконечным пространством состояний в основании алгоритма Q-Learning лежит нейронная сеть.
Она обрабатывает вектор параметров среды и выдает оценки суммарных наград для каждого действия.

<!-- Функция ошибки, на которой учится сеть -->

Такой подход называется Deep Q Learning (DQN). <!-- Есть ли статьи про DQN -->
У алгоритма есть недостаток - излишняя оптимистичность.
Оценки алгоритма после обучения оказываются сильное выше, чем получаемые агентом награды.

Для решения этой проблемы предложена модификация алгоритма Double DQN.
В ней добавляется еще одна нейросеть.
Она медленнее обучается, тем самым дольше остается пессимистичной.

<!-- Модифицированная функция ошибки -->

На практике оценки модифицированного алгоритма оказываются сильно ближе к полученным. <!-- Ссылка на статью -->

### Система наград

В игре нет заранее заданной системы наград, поэтому ее нужно выбрать так, чтобы максимизация суммы наград за действия приближала агента к цели проекта.
Цель проекта обыгрывать человека в Симуляторе владельца продукта.
Обыграть человека значит подняться выше любого человека в лидерборде.
В лидерборде главным параметром является количество завершенных спринтов до победы.
Чем меньше спринтов тем, лучше.

Первая идея: штрафовать за каждый спринт.
За остальные действия не давать ни штрафа ни награды.

Результат: агент научился максимально быстро проигрывать.

Вторая идея: награждать за валидные действия и сильно награждать за победу.
В этом случае коэффициент дисконтирования мотивировал играть быстрее.

<!-- Не сходилось, но играл дольше -->
При такой системе наград агент научился дольше играть.
В частности достигать спринта, в котором заканчивается выплата кредита.
После выплаты кредита в игре появляются карточки багов и тех. долга, которые усложняют процесс игры.

<!-- Разделить игру на этапы; Главный инсайт - спамить спринты в конце -->
Чтобы "помочь" DDQN выиграть в игре было решено разделить игру на этапы.
Первый этап - обучение, в котором нужно исследовать, декомпозировать и релизить одну карточку.
На этом этапе действия тривиальны и агент без труда учится выполнять задачу за минимальное количество спринтов.

Второй этап - самый сложный, выплата кредита.
В нем нужно довести до релиза множество задач, постепенно накапливая количество пользователей и их лояльность.
На этом этапе агент ограничен по количеству спринтов и должен максимизировать набор параметров, такие как благосостояние и удововлетворенность пользователей.

Третий этап - набор необходимого капитала.
После выплаты кредита, денег за спринт становится больше, но появляются карточки багов и технического долга.
Они усложняют решение задач и увеличивают отток пользователей.

Оказалось, что при достаточно хорошей игре во втором этапе, накопленных денег и пользователей достаточно, чтобы делать пустые спринты, то есть спринты без задач.
Таким образом количество пользователей убывает с каждым спринтом, но даже так дохода достаточно, чтобы накопить миллион.

<!-- Запретить не валдиные действия -->
В игре доступные действия определяются её состоянием.
Например, исследование, которое создает карточку, стоит $80k.
Если сейчас денег меньше, то провести исследование нельзя.
Алгоритму это может подсказать среда, после применения действия среда не меняется, а награда за действие будет отрицательной.
Чтобы учитывать недоступные действия агент должен выучить закономерности и зависимости действий от среды.

Гипотеза: если агент будет выбирать только из доступных действий, обучение ускорится и достигнет качественно лучшего результата.
Эксперимент поставлен так: обучаться агент будет на первых 35 спринтах.
Если игра не закончена, доигрывание будет пустыми спринтами.
Результат: агент научился выигрывать с определенным шансом.
<!-- Есть картинка! -->

<!-- Поменяли систему наград -->
Гипотеза: если агента больше награждать за потенциал, который можно потратить жадным доигрыванием, агент накопит больше капитала к 35-му спринту.
Таким образом шанс выиграть после работы агента станет сильно выше.
Результат: получилось увеличить вероятность победы и не увеличить победный спринт!
<!-- Есть данные -->

### Лидерборд
<!-- Попадание в лидерборд -->
Для достижения цели проекта нужно добиться того, чтобы обученный агент попал в лидерборд.
Для попадания в лидерборд агент должен совершать действия в веб версии игры.
Автоматизировать работу с браузером позволяет библиотека Silenium [<!-- Ссылка -->].
После загрузки страницы появляется задача считывания состояния игры, чтобы агент смог выбрать оптимальное действие.
По техническим причинам в браузере игра описывается только картинкой.
Для работы с изображениями подойдет openCV [<!-- Ссылка -->] - это мощный инструмент компьютерного зрения.
В фиксированных местах расположены необходимые данные, такие как номер спринта, текущий капитал.
Поэтому считывание цифр с помощью сравнения с шаблоном - самый простой подходящий вариант.
Из-за специфики игры описанный метод ведет себя нестабильно.

После считывания состояния, передачи его агенту, появляется действие, которое нужно совершить.
Если действие - релиз или покупка исследования, проблем почти нет.
Нужно кликнуть по заранее заданному положению на экране.
Если же действие - перенос карточки из беклога в спринт, нужно либо запомнить положение карточки, либо найти ее на экране заново.
Так же важно отловить момент выплаты кредита, ведь после него нужно доигрывать пустыми спринтами.

## Оценка и сравнение результатов

До лидерборда алгоритм еще не добрался.
Агент обучается на логике игры, переписанной с Godot на Python.
Эта логика перенесена с использованием оригинального кода.

При валидации алгоритма - шанс выиграть в игре составляет 68 процентов.
При этом минимальный выигрышный спринт - 47.
Это ровно первое место в текущем лидерборде.

Не очень большая нейронная сеть может играть на уровне человека в симулятор владельца продукта!

## Выводы и возможные направления дальнейших действий

Для улучшений результатов алгоритма можно попробовать увеличить размер нейронной сети лежащей в основе алгоритма.
Или же попробовать применить более сложные алгоритмы RL.

Можно развить симулятор владельца продукта, немного приблизив его к реальности.
И с помощью текущих наработок адаптировать ИИ побеждать в следующий версии игры.
Возможно, таким итеративным образом можно достичь уровень ИИ способного быть владельцем настоящего продукта.