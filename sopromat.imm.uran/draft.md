# Поиск оптимальной стратегии игры в симуляторе владельца продукта с помощью Deep RL (DDQN)
<!-- Применение DDQN в симуляторе владельца продукта: оптимизация стратегий с помощью обучения с подкреплением (Chat GPT 4 version)-->

## Введение:

### Что такое Reinforcement Learning?

В обучении с подкреплением (reinforcement learning)
алгоритм учится 
принимать последовательность решений
в некоторой среде 
с целью максимизации суммарной награды.

* Что за симулятор владельца продукта?



## Актуальность

* Машинное обучение помогает человечеству автоматизировать и оптимизировать процессы
* В разработке очень важную роль играет управление рабочими задачами в продукте
* Давайте попробуем автоматизировать управление продукта на упрощенной модели

## Обзор существующих аналогов

### Есть ли еще игры - симуляторы владения продукта
Game Dev Tycoon - игра, в которой вы создаете и развиваете свою собственную игровую компанию, начиная с 80-х годов.
<!--- Создается множество игр. Сами игры описыаются характеристиками тема, жанр, масштаб, технологии -->
Software Inc. - симулятор разработки программного обеспечения, в котором вы создаете и управляете своей собственной IT-компанией.
<!-- Очень сильный упор на реализм. -->

* Есть ли алгоритмы, которые уже хорошо играют в выбранную игру?
Точно нет алгоритмов обыгрывающих человека в Product Owner Simulator.

## Описание процесса достижение цели

Для успешного применения RL алгоритма к игре следует представить ее в виде среды с системой наград.
Среда должна иметь свойстово марковости, то есть её поведение зависит только от текущего состояния и не зависит от предыдущих.

### Среда
Рассматрим среду с которой будет взаимодействовать агент.
У среды можно определить пространство состояний, важна его конечность.
<!-- Скачек от пространства состояний к параметрам -->
Например в выбранной игре один из параметров, описывающих состояние среды, - это номер спринта, который на практике не привышает ста.
Но в набор параметров так же входит количестов пользователей и их лояльность, которые ведут себя как действительные числа.
Хоть в компьютере числа хранятся с конечной точностью, пречисленные факторы делают пространстов игры необъятным, поэтому можно считать его бесконечным.

Аналогично определяется пространство действий.
В случае Симулятора владельца продукта есть кончное число заранее заданных действий, например: начать спринт, преобрести работника или начать исследование.
В ходе игры можно передвигать карточки, количество которых величина динамическая.
В рамках игры количество карточек ограничено большим конечным числом, поэтому пространстов действий конечно.

В данной среде присутвуют случайные собития, например, прирост пользователей благодаря релизу задачи или возникновения бага при релизе.

В игре можно выйграть, набрать миллион условных денег или проиграть: опуститься до нуля.
Таким образом среда обладает свойством эпизодичности.
Некоторые среды таким свойством не обладают, например, управление роботом или торговля на бирже.

### Агент

Агент представляет собой алгоритм, по состоянию среды выдающий оптимальное действие.
Существуем множество способов получения такого агента: Cross-Entropy, Policy Iteration, Monte-Carlo, SARSA, Q-Learning. <!-- Ссылка на ods.ai -->
Из перечисленных вариантов был выбран Q-Learning, он хорошо показывает себя в бесконечных средах с конечным пространством действий.
Учитывает стохастику и сходится быстрее чем Cross-Entropy.
<!-- Алгоритм не учитывает эпизодичность среды, это может замедлить обучени алгоритма, но не повлияет на его сходимость. -->

Для среды с бесконечным пространством состояний в основании алгоритма Q-Learning лежит нейронная сеть.
Она обрабатывет вектор параметров среды и выдает оценки суммарных наград для каждого действия.

<!-- Функция ошибки, на которой учится сеть -->

Такой подход называется Deep Q Learning (DQN). <!-- Есть ли статьи про DQN -->
У алгоритма есть недостаток - излишняя оптимистичность.
Оценки алгоритма после обучения оказываются сльно выше, чем получаемые агентом награды.

Для решения этой проблемы предложена модификация алгоритма Double DQN.
В ней добовляется еще одна нейросеть.
Она медленнее обучается, тем самым дольше остается писсимистичной.

<!-- Модифицированная функция ошибки -->

На практике оценки модифицированного алгоритма оказываются сильно ближе к полученным. <!-- Ссылка на статью -->

### Система наград

В игре нет заранее заданной системы наград, поэтому ее нужно выбрать так, чтобы максимазация суммы наград за действия приближала агента к цели проекта.
Цель проекты обыгрывать человека в Симуляторе владельца продукта.
Обыграть человека значит поднятся выше любого человека в лидерборде.
В лидерборде главным параметром является количество завершенных спринтов до победы.
Чем меньше спринтов тем, лучше.

Первая идея: штрафовать за каждый спринт.
За остальные действия не давать ни штрафа ни награды.

Результат: агент научился максимально быстро проигрывать.

Вторая идея: награждать за валидные действия и сильно награждать за победу.
В этом случае коэфицент дисконтирования мотивировал играть быстрее.

<!-- Не сходилось, но играл дольше -->
При такой системе наград агент научился дольше играть.
В частности достигать спринта, в котором заканчивается выплата кредита.
После выплаты кредита в игре появляются карточки багов и тех. долга, которые усложняют процесс игры.

<!-- Разделить игру на этапы; Главный инсайт - спамить спринты в конце -->
Чтобы "помочь" DDQN выйграть в игре было решено разделить игру на этапы.
Первый этап - обучение, в котором нужно купить, декомпозировать и релизить одну карточку.
На этом действия тривиальны и агент без труда учится выполнять задачу за минимальное количество спринтов.

Второй этап - самый сложный, выплата кредита. В нем нужно довести до релиза множество задач, постепенно накапливая количество пользователей и их лояльность.
На этом этапе агент ограничен по количеству спринтов и должен максимизировать набор параметров, такие как благосостояние и называнные выше.

Третий этап - набор необходимого капитала.
Оказалось, что при достаточно хорошей игре во втором этапе, денег и пользователей достаточно, чтобы делать пустые спринты, то есть спринты без задач.
Таким образом количество пользователей убывает с каждым спринтом, но даже так дохода достаточно чтобы накопить миллион.

<!-- Запретить не валдиные действия -->
В игре доступные действия определяются её состоянием.
Например исследование, которое создает карточку стоит $80k.
Если сейчас денег меньше, то провести исследование нельзя.
Алгоритму это может подсказать среда, после применения действия среда не меняется, а награда за действие будет отрицательной.
Чтобы учитывать недоступные действия агент должен выучить закономерности зависимости действий от среды.

Гипотиза: если агент будет выбирать только из доступных действий, обучение ускорится и достигнет какечственного лучшего результата.
Експеримент поставлен так: обучаться агент будет на первых 35 спринтах.
Если игра не закочена, доигрывание будет пустыми спринтами.
Результат: агент научился выигрывать с определенным шансом.
<!-- Есть картинка! -->

<!-- Поменяли систему наград -->

<!-- Попадание в лидерборд -->

## Оценка и сравнение результатов

## Выводы и возможные направления дальнейших действий