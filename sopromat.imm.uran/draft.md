# Поиск оптимальной стратегии игры в симуляторе владельца продукта с помощью Deep RL (DDQN)
<!-- Применение DDQN в симуляторе владельца продукта: оптимизация стратегий с помощью обучения с подкреплением (Chat GPT 4 version)-->

## Введение:

### Что такое Reinforcement Learning?

В обучении с подкреплением (reinforcement learning)
алгоритм учится 
принимать последовательность решений
в некоторой среде 
с целью максимизации суммарной награды.

* Что за симулятор владельца продукта?



## Актуальность

* Машинное обучение помогает человечеству автоматизировать и оптимизировать процессы
* В разработке очень важную роль играет управление рабочими задачами в продукте
* Давайте попробуем автоматизировать управление продукта на упрощенной модели

## Обзор существующих аналогов

### Есть ли еще игры - симуляторы владения продукта
Game Dev Tycoon - игра, в которой вы создаете и развиваете свою собственную игровую компанию, начиная с 80-х годов.
<!--- Создается множество игр. Сами игры описыаются характеристиками тема, жанр, масштаб, технологии -->
Software Inc. - симулятор разработки программного обеспечения, в котором вы создаете и управляете своей собственной IT-компанией.
<!-- Очень сильный упор на реализм. -->

* Есть ли алгоритмы, которые уже хорошо играют в выбранную игру?
Точно нет алгоритмов обыгрывающих человека в Product Owner Simulator.

## Описание процесса достижение цели

Для успешного применения RL алгоритма к игре следует представить ее в виде среды с системой наград.
Среда должна иметь свойстово марковости, то есть её поведение зависит только от текущего состояния и не зависит от предыдущих.

### Среда
Рассматрим среду с которой будет взаимодействовать агент.
У среды можно определить пространство состояний, важна его конечность.
<!-- Скачек от пространства состояний к параметрам -->
Например в выбранной игре один из параметров, описывающих состояние среды, - это номер спринта, который на практике не привышает ста.
Но в набор параметров так же входит количестов пользователей и их лояльность, которые ведут себя как действительные числа.
Хоть в компьютере числа хранятся с конечной точностью, пречисленные факторы делают пространстов игры необъятным, поэтому можно считать его бесконечным.

Аналогично определяется пространство действий.
В случае Симулятора владельца продукта есть кончное число заранее заданных действий, например: начать спринт, преобрести работника или начать исследование.
В ходе игры можно передвигать карточки, количество которых величина динамическая.
В рамках игры количество карточек ограничено большим конечным числом, поэтому пространстов действий конечно.

В данной среде присутвуют случайные собития, например, прирост пользователей благодаря релизу задачи или возникновения бага при релизе.

В игре можно выйграть, набрать миллион условных денег или проиграть: опуститься до нуля.
Таким образом среда обладает свойством эпизодичности.
Некоторые среды таким свойством не обладают, например, управление роботом.

### Агент

Агент представляет собой алгоритм, по состоянию среды выдающий оптимальное действие.
Существуем множество способов получения такого агента: Cross-Entropy, Policy Iteration, Monte-Carlo, SARSA, Q-Learning. <!-- Ссылка на ods.ai -->
Из перечисленных вариантов был выбран Q-Learning, он хорошо показывает себя в бесконечных средах с конечным пространством действий.
Учитывает стохастику и сходится быстрее чем Cross-Entropy.
<!-- Алгоритм не учитывает эпизодичность среды, это может замедлить обучени алгоритма, но не повлияет на его сходимость. -->

Для среды с бесконечным пространством состояний в основании алгоритма Q-Learning лежит нейронная сеть. Она обрабатывет вектор параметров среды и выдает оценки суммарных наград для каждого действия.

<!-- Функция ошибки, на которой учится сеть -->

Такой подход называется Deep Q Learning (DQN). <!-- Есть ли статьи про DQN -->
У алгоритма есть недостаток - излишняя оптимистичность.
Оценки алгоритма после обучения оказываются сльно выше, чем получаемые агентом награды.

Для решения этой проблемы предложена модификация алгоритма Double DQN.
В ней добовляется еще одна нейросеть.
Она медленнее обучается, тем самым дольше остается писсимистичной.

<!-- Модифицированная функция ошибки -->

На практике оценки модифицированного алгоритма оказываются сильно ближе к полученным. <!-- Ссылка на статью -->

### Система наград


## Оценка и сравнение результатов

## Выводы и возможные направления дальнейших действий